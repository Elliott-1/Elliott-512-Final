{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# US EPA Air Quality System API Example\n",
    "This example illustrates how to request data from the US Environmental Protection Agency (EPA) Air Quality Service (AQS) API. This is a historical API and does not provide real-time air quality data. The [documentation](https://aqs.epa.gov/aqsweb/documents/data_api.html) for the API provides definitions of the different call parameter and examples of the various calls that can be made to the API.\n",
    "\n",
    "This notebook works systematically through example calls, requesting an API key, using 'list' to get various IDs and parameter values, and using 'daily summary' to get summary data that meets specific condistions. The notebook contains example function definitions that could be reused in other code. In general, the notebook explains each step along the way, referring back to possible output. Some of the explanations are tailored to the specific example requests of the API. Changing values to explore the results of the API is probably useful, but that will result in some explanations being out of sync with the outputs.\n",
    "\n",
    "The US EPA was created in the early 1970's. The EPA reports that they only started broad based monitoring with standardized quality assurance procedures in the 1980's. Many counties will have data starting somewhere between 1983 and 1988. However, some counties still do not have any air quality monitoring stations. The API helps resolve this by providing calls to search for monitoring stations and data using either station ids, or a county designation or a geographic bounding box. This example code provides examples of the county based and bounding box based API calls. Some [additional information on the Air Quality System can be found in the EPA FAQ](https://www.epa.gov/outdoor-air-quality-data/frequent-questions-about-airdata) on the system.\n",
    "\n",
    "The end goal of this example is to get to some values that we might use for the Air Quality Index or AQI. You might see this reported on the news, most often around smog, but more frequently with regard to smoke. The AQI index is meant to tell us something about how healthy or clean the air is on any day. The AQI is actually a somewhat complext measure. When I started this example I looked up [how to calculate the AQI](https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf) so that I would know roughly what goes into that value.\n",
    "\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - August 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "#    These are standard python modules\n",
    "#\n",
    "#import json, time, urllib.parse\n",
    "import json, time\n",
    "#\n",
    "#    The 'requests' module is a distribution module for making web requests. If you do not have it already, you'll need to install it\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#\n",
    "#    This is the root of all AQS API URLs\n",
    "#\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "#\n",
    "#    These are some of the 'actions' we can ask the API to take or requests that we can make of the API\n",
    "#\n",
    "#    Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "#\n",
    "#    List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "#\n",
    "#    Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    It is always nice to be respectful of a free data resource.\n",
    "#    We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "#\n",
    "#\n",
    "#    This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "#    above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "#    function - or they can be set in a copy of the template and passed in with the template.\n",
    "# \n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n",
    "\n",
    "#   Given the set of sensor codes, now we can create a parameter list or 'param' value as defined by the AQS API spec.\n",
    "#   It turns out that we want all of these measures for AQI, but we need to have two different param constants to get\n",
    "#   all seven of the code types. We can only have a max of 5 sensors/values request per param.\n",
    "#\n",
    "#   Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "#\n",
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3. Making a daily summary request\n",
    "\n",
    "The function below is designed to encapsulate requests to the EPA AQS API. When calling the function one should create/copy a parameter template, then initialize that template with values that won't change with each call. Then on each call simply pass in the parameters that need to change, like date ranges.\n",
    "\n",
    "Another function below provides an example of extracting values and restructuring the response to make it a little more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the daily summary request. Daily summary provides a daily summary value for each sensor being requested\n",
    "#    from the start date to the end date. \n",
    "#\n",
    "#    Like the two other functions, this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "#    parameters. If function parameters are provided, those take precedence over any parameters from the request template.\n",
    "#\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "        \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of the daily summary response is a bit verbose with lots of repeated values. What we'll do is create a data structure that relies on a hierarchical context to summarize the data.\n",
    "\n",
    "The two responses (for Bend, OR) show that not every monitoring site produces values. As well, it looks like the monitoring sites only produce values for particulates and not for gaseous pollutants.\n",
    "\n",
    "The next function takes the response and a set of fields that should be extracted for their data values. The code assumes those fields are available. If there are missing values something could certainly go wrong. The function creates a summary for each monitoring site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This is a list of field names - data - that will be extracted from each record\n",
    "#\n",
    "EXTRACTION_FIELDS = ['sample_duration','observation_count','arithmetic_mean','aqi']\n",
    "\n",
    "#\n",
    "#    The function creates a summary record\n",
    "def extract_summary_from_response(r=None, fields=EXTRACTION_FIELDS):\n",
    "    \n",
    "    ## the result will be structured around monitoring site, parameter, and then date\n",
    "    result = dict()\n",
    "    #data = r[\"Data\"]\n",
    "    for record in r:\n",
    "        # make sure the record is set up\n",
    "        site = record['site_number']\n",
    "        param = record['parameter_code']\n",
    "        #date = record['date_local']    # this version keeps the respnse value YYYY-\n",
    "        date = record['date_local'].replace('-','') # this puts it in YYYYMMDD format\n",
    "        if site not in result:\n",
    "            result[site] = dict()\n",
    "            result[site]['local_site_name'] = record['local_site_name']\n",
    "            result[site]['site_address'] = record['site_address']\n",
    "            result[site]['state'] = record['state']\n",
    "            result[site]['county'] = record['county']\n",
    "            result[site]['city'] = record['city']\n",
    "            result[site]['pollutant_type'] = dict()\n",
    "        if param not in result[site]['pollutant_type']:\n",
    "            result[site]['pollutant_type'][param] = dict()\n",
    "            result[site]['pollutant_type'][param]['parameter_name'] = record['parameter']\n",
    "            result[site]['pollutant_type'][param]['units_of_measure'] = record['units_of_measure']\n",
    "            result[site]['pollutant_type'][param]['method'] = record['method']\n",
    "            result[site]['pollutant_type'][param]['data'] = dict()\n",
    "        if date not in result[site]['pollutant_type'][param]['data']:\n",
    "            result[site]['pollutant_type'][param]['data'][date] = list()\n",
    "        \n",
    "        # now extract the specified fields\n",
    "        extract = dict()\n",
    "        for k in fields:\n",
    "            if str(k) in record:\n",
    "                extract[str(k)] = record[k]\n",
    "            else:\n",
    "                # this makes sure we always have the requested fields, even if\n",
    "                # we have a missing value for a given day/month\n",
    "                extract[str(k)] = None\n",
    "        \n",
    "        # add this extraction to the list for the day\n",
    "        result[site]['pollutant_type'][param]['data'][date].append(extract)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below, multi_year_data_pull, is used to pull multiple years of EPA data. It has the option to save the raw files or continue to have them processed using the function above. This function was adapted heavily from the tutorial code provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_year_data_pull(first_year, last_year, city_fips, save=False):\n",
    "    \"\"\"\n",
    "    Takes in the starting and ending years, the fips code for a given city, \n",
    "    and whether to save the raw file and outputs the EPA AQI information for both gaseous and particulate pollutants.\n",
    "\n",
    "    Parameters:\n",
    "        - first_year: The starting year to begin requesting data\n",
    "        - last_year: The last year to request data\n",
    "        - city_fips: The fips code for a given city\n",
    "        - save: Whether to save the raw JSONs\n",
    "\n",
    "    Returns two processed JSONs, one for gaseous and one for particulate. \n",
    "    \"\"\"\n",
    "    request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "    request_data['email'] = USERNAME\n",
    "    request_data['key'] = APIKEY\n",
    "    request_data['param'] = AQI_PARAMS_GASEOUS\n",
    "    request_data['state'] = city_fips[:2]\n",
    "    request_data['county'] = city_fips[2:]\n",
    "    \n",
    "    # Define the start and end year range\n",
    "    start_year = first_year\n",
    "    end_year = last_year\n",
    "    \n",
    "    # Initialize lists to hold combined data for each pollutant type\n",
    "    combined_gaseous_data = []\n",
    "    combined_particulate_data = []\n",
    "    \n",
    "    # Loop through each year in the range\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        begin_date = f\"{year}0501\"\n",
    "        end_date = f\"{year}1031\"\n",
    "        \n",
    "        # Request daily summary data for gaseous pollutants\n",
    "        gaseous_aqi = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date)\n",
    "        if gaseous_aqi[\"Header\"][0]['status'] == \"Success\":\n",
    "            combined_gaseous_data.extend(gaseous_aqi['Data'])\n",
    "        elif gaseous_aqi[\"Header\"][0]['status'].startswith(\"No data\"):\n",
    "            print(f\"No data for gaseous pollutants in {year}.\")\n",
    "        else:\n",
    "            print(f\"Error in gaseous data request for {year}: {json.dumps(gaseous_aqi, indent=4)}\")\n",
    "        \n",
    "        # Adjust request data for particulate pollutants\n",
    "        request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "        \n",
    "        # Request daily summary data for particulate pollutants\n",
    "        particulate_aqi = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date)\n",
    "        if particulate_aqi[\"Header\"][0]['status'] == \"Success\":\n",
    "            combined_particulate_data.extend(particulate_aqi['Data'])\n",
    "        elif particulate_aqi[\"Header\"][0]['status'].startswith(\"No data\"):\n",
    "            print(f\"No data for particulate pollutants in {year}.\")\n",
    "        else:\n",
    "            print(f\"Error in particulate data request for {year}: {json.dumps(particulate_aqi, indent=4)}\")\n",
    "\n",
    "    if save: \n",
    "        # Save the gaseous data to a JSON file\n",
    "        with open(f\"combined_gaseous_aqi_data_{start_year}_{end_year}.json\", \"w\") as f:\n",
    "            json.dump(combined_gaseous_data, f, indent=4)\n",
    "        \n",
    "        # Save the particulate data to a separate JSON file\n",
    "        with open(f\"combined_particulate_aqi_data_{start_year}_{end_year}.json\", \"w\") as f:\n",
    "            json.dump(combined_particulate_data, f, indent=4)\n",
    "    \n",
    "    processed_gaseous_data = extract_summary_from_response(combined_gaseous_data)\n",
    "    processed_particulate_data = extract_summary_from_response(combined_particulate_data)\n",
    "\n",
    "    return processed_gaseous_data, processed_particulate_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below, process_extract, is used to take in the processed JSONs, extract the AQI data, and create a dataframe that gives AQI data per day for a given dataset\n",
    "\n",
    "I also make the decision to average the AQI values if there are multiple for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_extract(data):\n",
    "    \"\"\"\n",
    "    Takes in a JSON file and returns the AQI information \n",
    "\n",
    "    Parameters:\n",
    "        - data: A JSON containing information about the AQI\n",
    "\n",
    "    Returns a dataframe where the rows are days, and the columns are pollutants. \n",
    "    The values are the average AQI for a given day\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold data for each pollutant and day\n",
    "    daily_data = {}\n",
    "\n",
    "    # Iterate through each main entry in data\n",
    "    for main_key, main_data in data.items():\n",
    "        pollutants_data = main_data['pollutant_type']\n",
    "        \n",
    "        # Iterate over each pollutant type\n",
    "        for pollutant_id, pollutant_info in pollutants_data.items():\n",
    "            pollutant_name = pollutant_info['parameter_name']\n",
    "            \n",
    "            # Access daily observations for this pollutant\n",
    "            for date, records in pollutant_info['data'].items():\n",
    "                \n",
    "                # Extract AQI values, excluding `None`s\n",
    "                aqi_values = [entry['aqi'] for entry in records if entry['aqi'] is not None]\n",
    "                \n",
    "                # Calculate mean AQI or assign `np.nan` if all values are `None`\n",
    "                mean_aqi = np.mean(aqi_values) if aqi_values else np.nan\n",
    "                \n",
    "                # Populate the dictionary: each date has a dictionary of pollutants\n",
    "                if date not in daily_data:\n",
    "                    daily_data[date] = {}\n",
    "                \n",
    "                # Store the mean AQI for the pollutant\n",
    "                daily_data[date][pollutant_name] = mean_aqi\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(daily_data, orient='index')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, get_max_aqi, handles the entire extraction process of daily AQI information. It uses the two helper function above to get data from both gaseuous and particulate pollutants, get the daily AQI values and output them to a dataframe.\n",
    "\n",
    "I also make the decision that I'm going to be the maximum AQI for a given day of all pollutants. Based on the research I found, there wasn't a good way to assess AQI values between pollutants. Therefore, I figured the max would likely represent the most sensitive measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a beginning and ending year and a tuple entry where the key is a city and the value is the fips ID, \n",
    "# will return a processed dataframe with AQI levels for each pollutant\n",
    "# Includes a column with the maximum AQI recorded for each day of fire season between the two years.\n",
    "\n",
    "def get_max_aqi(first_year, last_year, city_info, save_raw = False, save_dataframe = False, loc = \"data\"):\n",
    "    \"\"\"\n",
    "    Takes in the starting and ending years, the fips code for a given city, \n",
    "    and whether to save the raw file, whether to save the final dataframe, and what folder to save it to\n",
    "    and outputs a dataframe giving the AQI for each pollutant for each day.\n",
    "\n",
    "    Parameters:\n",
    "        - first_year: The starting year to begin requesting data\n",
    "        - last_year: The last year to request data\n",
    "        - city_info: A list where the first \n",
    "          element is the city name and the last element is the fips code. Both are strings\n",
    "        - save_raw: Whether to save the raw JSONs\n",
    "        - save_dataframe: Whether to save the final dataframe\n",
    "        - loc: Which folder to save the final dataframe if save_dataframe = True\n",
    "\n",
    "    Returns a combined dataframe - where the rows are days, the columns pollutants, and the values are AQI values.\n",
    "    \"\"\"\n",
    "    gaseous, particulate = multi_year_data_pull(first_year, last_year, city_info[1], save_raw)\n",
    "    \n",
    "    # Process each extract\n",
    "    \n",
    "    df_gaseous = process_extract(gaseous)\n",
    "    df_particulate = process_extract(particulate)\n",
    "    \n",
    "    # Combine the two dataframes by joining them on the date index\n",
    "    df_combined = pd.concat([df_gaseous, df_particulate], axis=1)\n",
    "    \n",
    "    # Convert the index to datetime format, sort, and display the final DataFrame\n",
    "    df_combined.index = pd.to_datetime(df_combined.index, format='%Y%m%d')\n",
    "    df_combined = df_combined.sort_index()\n",
    "\n",
    "    df_combined['max_aqi'] = df_combined.max(axis=1)\n",
    "    if save_dataframe:\n",
    "        df_combined.to_csv(f\"{loc}/AQI_{city_info[0]}_{first_year}_{last_year}.csv\")\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates my city information for my given city, then runs the files. I choose to just save the final dataframe and not the raw JSON files. Even though the assignment requests EPA data as far back as possible (theoretically 1970s), I start in 1980 because there isn't any data until 1986."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for particulate pollutants in 1980.\n",
      "No data for gaseous pollutants in 1981.\n",
      "No data for particulate pollutants in 1981.\n",
      "No data for gaseous pollutants in 1982.\n",
      "No data for particulate pollutants in 1982.\n",
      "No data for gaseous pollutants in 1983.\n",
      "No data for particulate pollutants in 1983.\n",
      "No data for gaseous pollutants in 1984.\n",
      "No data for particulate pollutants in 1984.\n",
      "No data for gaseous pollutants in 1985.\n",
      "No data for particulate pollutants in 1985.\n"
     ]
    }
   ],
   "source": [
    "sioux_falls_info = [\"Sioux_Falls\", \"46099\"]\n",
    "sioux_falls_1980_2021 = get_max_aqi(1980, 2021, sioux_falls_info, save_raw=False, save_dataframe=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
